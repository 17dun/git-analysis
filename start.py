import sys
import os
import requests
import base64
import markdown
import re
from bs4 import BeautifulSoup

def clean_markdown_to_text(md_content):
    """
    Converts Markdown content to clean plain text, removing HTML tags and other formatting.
    åŒæ—¶åº”ç”¨å†…å®¹ç²¾ç®€ç­–ç•¥ä»¥å‡å°‘æœ€ç»ˆæ–‡æœ¬ä½“ç§¯ã€‚
    """
    # ç¬¬ä¸€æ­¥ï¼šç²¾ç®€ Markdown å†…å®¹
    md_content = simplify_readme_content(md_content)

    try:
        html = markdown.markdown(md_content)
    except Exception as e:
        print(f"è­¦å‘Šï¼šMarkdownè½¬æ¢HTMLæ—¶å‡ºé”™: {e}", file=sys.stderr)
        html = md_content

    try:
        soup = BeautifulSoup(html, "html.parser")
        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose()
        text = soup.get_text(separator='\n', strip=True)
        print("Markdown å†…å®¹å·²æˆåŠŸæ¸…æ´—ä¸ºçº¯æ–‡æœ¬ã€‚", file=sys.stderr)
        return text
    except Exception as e:
        print(f"è­¦å‘Šï¼šHTMLæ¸…æ´—æ—¶å‡ºé”™: {e}", file=sys.stderr)
        return md_content

def simplify_readme_content(md_content):
    """
    å¯¹ README å†…å®¹åº”ç”¨ç²¾ç®€ç­–ç•¥ï¼Œå»é™¤å†—ä½™å’Œä¸å¤ªé‡è¦çš„å†…å®¹ã€‚
    """
    lines = md_content.split('\n')
    result_lines = []
    skip_until_next_section = False
    in_important_section = False
    section_depth = 0
    kept_sections = set()

    # å®šä¹‰è¦ä¿ç•™çš„ç« èŠ‚å…³é”®è¯ï¼ˆä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼‰
    important_sections = [
        'quick start', 'getting started', 'installation', 'install', 'ä»‹ç»', 'ç®€ä»‹',
        'feature', 'ç‰¹æ€§', 'åŠŸèƒ½', 'overview', 'æ¦‚è¿°'
    ]

    # å®šä¹‰è¦è·³è¿‡çš„ç« èŠ‚å…³é”®è¯
    skip_sections = [
        'news', 'release', 'changelog', 'history', 'version',
        'faq', 'question', 'troubleshooting', 'star history',
        'contribution', 'contributing', 'license', 'acknowledgement',
        'æ–°é—»', 'å‘å¸ƒ', 'ç‰ˆæœ¬', 'å¸¸è§é—®é¢˜', 'è´¡çŒ®', 'è®¸å¯', 'è‡´è°¢'
    ]

    # æ ‡è®°æ˜¯å¦å·²ç»ä¿ç•™äº†ç‰¹æ€§åˆ—è¡¨ï¼ˆé¿å…é‡å¤ï¼‰
    feature_kept = False

    for i, line in enumerate(lines):
        # æ£€æµ‹ç« èŠ‚æ ‡é¢˜
        if line.startswith('#'):
            # é‡ç½®è·³è¿‡æ ‡è®°
            skip_until_next_section = False

            # è·å–ç« èŠ‚æ ‡é¢˜ï¼ˆå»é™¤ # å·ï¼‰
            section_title = line.lstrip('#').strip().lower()

            # åˆ¤æ–­æ˜¯å¦æ˜¯è¦è·³è¿‡çš„ç« èŠ‚
            if any(skip_word in section_title for skip_word in skip_sections):
                skip_until_next_section = True
                continue

            # åˆ¤æ–­æ˜¯å¦æ˜¯é‡è¦ç« èŠ‚ï¼ˆåªä¿ç•™å‰3ä¸ªé‡è¦ç« èŠ‚ï¼‰
            if any(imp_word in section_title for imp_word in important_sections):
                if len([s for s in kept_sections if any(imp_word in s.lower() for imp_word in important_sections)]) < 3:
                    kept_sections.add(section_title)
                    in_important_section = True
                else:
                    skip_until_next_section = True
                    continue

            # æ£€æµ‹ç‰¹æ€§åˆ—è¡¨ï¼ˆåªä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
            if 'feature' in section_title or 'ç‰¹æ€§' in section_title:
                if feature_kept:
                    skip_until_next_section = True
                    continue
                else:
                    feature_kept = True

        # è·³è¿‡è¢«æ ‡è®°çš„ç« èŠ‚
        if skip_until_next_section:
            continue

        # å»é™¤ badge å¾½ç« è¡Œ
        if re.match(r'^\s*\[!\[.*?\]\(.*?\)\]', line):
            continue

        # å»é™¤å¤šè¯­è¨€é“¾æ¥è¡Œï¼ˆå¦‚ ğŸ‡¨ğŸ‡³ ä¸­æ–‡ Â· ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªï¼‰
        if re.search(r'ğŸ‡¨ğŸ‡³|ğŸ‡¯ğŸ‡µ|ğŸ‡ªğŸ‡¸|ğŸ‡«ğŸ‡·|ğŸ‡¸ğŸ‡¦|ğŸ‡·ğŸ‡º|ğŸ‡®ğŸ‡³|ğŸ‡µğŸ‡¹', line):
            continue

        # å»é™¤çº¯é“¾æ¥è¡Œï¼ˆå¯¼èˆªç”¨ï¼‰
        if re.match(r'^\s*\[.*?\]\(.*?\)\s*Â·\s*\[.*?\]\(.*?\)', line):
            continue

        # å»é™¤ä»£ç å—ä¸­çš„é•¿ä»£ç ç¤ºä¾‹ï¼ˆè¶…è¿‡20è¡Œçš„ä»£ç å—ï¼‰
        if line.strip().startswith('```'):
            # æ£€æŸ¥ä»£ç å—é•¿åº¦
            code_start = i
            code_end = i
            for j in range(i + 1, len(lines)):
                if lines[j].strip().startswith('```'):
                    code_end = j
                    break
            # å¦‚æœä»£ç å—è¶…è¿‡15è¡Œï¼Œè·³è¿‡
            if code_end - code_start > 15:
                skip_until_next_section = True
                # æ·»åŠ ç®€çŸ­è¯´æ˜
                result_lines.append('(ä»£ç ç¤ºä¾‹å·²çœç•¥)')
                continue

        # å»é™¤è¡¨æ ¼ä¸­çš„ç¯å¢ƒå˜é‡é…ç½®ç­‰è¯¦ç»†è¡¨æ ¼ï¼ˆè¶…è¿‡5è¡Œçš„è¡¨æ ¼ï¼‰
        if line.startswith('|') and i > 0:
            table_start = i
            table_end = i
            for j in range(i, len(lines)):
                if not lines[j].startswith('|'):
                    table_end = j - 1
                    break
            # å¦‚æœè¡¨æ ¼è¶…è¿‡5è¡Œï¼Œè·³è¿‡
            if table_end - table_start > 5:
                skip_until_next_section = True
                continue

        # ä¿ç•™è¯¥è¡Œ
        result_lines.append(line)

    return '\n'.join(result_lines)

def search_github_repo(search_term):
    """
    Searches GitHub for the top repository and returns its full name and star count.
    """
    print(f"æ­£åœ¨æœç´¢å…³é”®è¯: {search_term}...", file=sys.stderr)
    url = f"https://api.github.com/search/repositories"
    headers = {"Accept": "application/vnd.github.v3+json"}
    params = {
        "q": search_term,
        "order": "desc"
    }
    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        data = response.json()
        if not data["items"]:
            print("é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä»“åº“ã€‚", file=sys.stderr)
            return None, None
        
        top_repo = data["items"][0]
        repo_full_name = top_repo["full_name"]
        star_count = top_repo["stargazers_count"]
        print(f"å·²æ‰¾åˆ°æ˜Ÿæ ‡æ•°æœ€é«˜çš„ä»“åº“: {repo_full_name} (Stars: {star_count})", file=sys.stderr)
        return repo_full_name, star_count
    except requests.exceptions.RequestException as e:
        print(f"é”™è¯¯ï¼šAPI è¯·æ±‚å¤±è´¥: {e}", file=sys.stderr)
        return None, None

def get_readme_content(repo_full_name):
    """
    Fetches the README content for a given repository.
    """
    print(f"æ­£åœ¨è·å– {repo_full_name} çš„ README æ–‡ä»¶...", file=sys.stderr)
    url = f"https://api.github.com/repos/{repo_full_name}/readme"
    headers = {"Accept": "application/vnd.github.v3+json"}
    
    try:
        response = requests.get(url, headers=headers)
        if response.status_code == 404:
            print(f"é”™è¯¯ï¼šä»“åº“ {repo_full_name} ä¸­æ²¡æœ‰æ‰¾åˆ° README æ–‡ä»¶ã€‚", file=sys.stderr)
            return None
        response.raise_for_status()
        
        data = response.json()
        content_base64 = data["content"]
        content_bytes = base64.b64decode(content_base64)
        readme_content = content_bytes.decode("utf-8")
        
        print("README æ–‡ä»¶å†…å®¹è·å–æˆåŠŸã€‚", file=sys.stderr)
        return readme_content
    except requests.exceptions.RequestException as e:
        print(f"é”™è¯¯ï¼šè·å– README å¤±è´¥: {e}", file=sys.stderr)
        return None

def save_content_to_output(filename, content):
    """
    Saves the content to the 'output' directory and returns the absolute path.
    """
    output_dir = "output"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"å·²åˆ›å»ºç›®å½•: {output_dir}", file=sys.stderr)

    file_path = os.path.join(output_dir, filename)
    absolute_path = os.path.abspath(file_path)
    
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {absolute_path}", file=sys.stderr)
        return absolute_path
    except IOError as e:
        print(f"é”™è¯¯ï¼šæ–‡ä»¶ä¿å­˜å¤±è´¥: {e}", file=sys.stderr)
        return None

def main():
    """
    Main function to run the script.
    """
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python start.py <æœç´¢å…³é”®è¯>", file=sys.stderr)
        sys.exit(1)
        
    search_term = sys.argv[1]
    
    repo_full_name, star_count = search_github_repo(search_term)
    
    if repo_full_name:
        readme_content = get_readme_content(repo_full_name)
        if readme_content:
            clean_text = clean_markdown_to_text(readme_content)
            
            # Construct filename with star count
            output_filename = f"STARS_{star_count}_{repo_full_name.replace('/', '_')}.txt"
            saved_path = save_content_to_output(output_filename, clean_text)
            
            if saved_path:
                print(saved_path)

if __name__ == "__main__":
    main()